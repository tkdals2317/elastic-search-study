## 4.4. 집계

검색을 수행한 뒤 그 결과를 집계하는 다양한 방법을 제공한다.

검색이 엘라스틱서치의 뿌리였다면 집계는 꽃이다.

집계 없이는 데이터 분석도 없다.

집계를 통해 비즈니스 통찰을 얻을 수 있으며, 많은 실제 문서를 데이터 기반으로 해결할 수 있다.

### 4.4.1 집계 기본

엘라스틱서치의 집계는 검색의 연장선이다.

집계의 대상을 추려낼 검색 조건을 검색 API에 담은 뒤 집계 조건을 추가해서 호출한다.

가장 간단한 형태인 sum 집계로 예를 들어보자.

아래 예제는 검색에 매칭된 문서를 대상으로 지정한 필드의 값을 모두 합친 값을 반환하는 집계 요청이다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0, // size를 0으로 지정하면 검색에 상위 매칭된 문서가 무엇인지 받아볼 수 없다.
  "query": {
    "term": {
      "currency": {
        "value": "EUR" // currency 필드의 값이 "EUR"과 일치하는 문서 찾기
      }
    }
  },
  "aggs": {
    "my-sum-aggregation-name": { // 집계로 나온 값의 이름 지정
      "sum": {
        "field": "taxless_total_price" // taxless_total_price의 합을 구한다.
      }
    }
  }
}
```

```
{
  "took": 10,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 4675,
      "relation": "eq"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "my-sum-aggregation-name": {
      "value": 350884.12890625
    }
  }
}
```

- 요청 본문에 aggs로 집계에 관련된 내용치 추가되었다.
- size를 0으로 지정하면 상위 매칭된 문서가 무엇인지 받아 볼 수 없다.
    - 하지만 이와 상관없이 검색 조건에 매치되는 모든 문서는 집계 작업에 사용된다.
    - 대부분의 집계 요청에서는 size를 0으로 지정하는 것이 이득이다.
    - 상위 문서의 내용을 수집해 모을 필요가 없고 점수를 계산하는 과정도 수행하지 않으므로 성능적으로 이득을 얻을 수 있다.
    - 캐시의 도움도 많이 받을 수 있다.
- 요청 한 번에 여러 집계를 요청할수도 있기 때문에 결과에서 이들을 구분할 수 있도록 집계에 이름을 붙혀야 한다. (위 쿼리에서는 `my-sum-aggregation-name` 라는 이름으로 집계의 이름을 지정했다.)
- 집계 작업은 검색 쿼리에 매칭되는 모든 문서에 대해 수행된다.
    - 과도한 집계는 전체 클러스터 성능을 급격히 저하 될 수 있다.
    - 키바나를 열어뒀을 때 시각화 그래프를 만들기 위해 과도한 집계를 하다가 장애가 날 수 있으니 주의하자
- 엘라스틱 집계는 크게 **메트릭 집계**, **버킷 집계**, **파이프라인 집계**로 분류된다.

### 4.4.2 메트릭 집계

메트릭 집계는 문서에 대한 산술적인 연산을 수행한다.

> **avg, max, min, sum**
>

avg, max, min, sum 집계는 검색에 매칭된 문서를 대상으로 지정한 필드의 값을 가져온 뒤 각각 평균, 최댓값, 최솟값, 합을 계산하여 반환한다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0,
  "query": {
    "term": {
      "currency": {
        "value": "EUR"
      }
    }
  },
  "aggs": {
    "my-avg-aggregation-name": {
      "**avg**": {
        "field": "taxless_total_price"
      }
    }
  }
}
```

사용법은 max, min, sum, avg 다 동일히다.

본문의 aggs 내부에 각 집계 이름을 지정하고 집계 방법(max, min, avg, sum)을 기술 한 후 해당 집계를할 필드를 지정하면된다.

> **stats 집계**
>

stats 집계는 지정한 필드의 평균, 최댓값, 최솟값, 합, 개수를 모두 계산해서 반환한다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0,
  "query": {
    "term": {
      "currency": {
        "value": "EUR"
      }
    }
  },
  "aggs": {
    "my-stats-aggregation-name": {
      "**stats**": {
        "field": "taxless_total_price"
      }
    }
  }
}
```

```
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 4675,
      "relation": "eq"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "my-stats-aggregation-name": {
      "count": 4675,
      "min": 6.98828125,
      "max": 2250,
      "avg": 75.05542864304813,
      "sum": 350884.12890625
    }
  }
}
```

- 여러 숫자 값을 한꺼번에 반환하는 메트릭 집계를 **다중 숫자 메트릭 집계**라고 부른다.

> **cardinality 집계**
>

cardinality 집계는 지정한 필드가 가진 고유한 값의 개수를 계산해 반환한다.

이 값은 HyperLogLog++ 알고리즘을 사용해 추정한 근사값이다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0,
  "query": {
    "term": {
      "currency": {
        "value": "EUR"
      }
    }
  },
  "aggs": {
    "my-cardianality-aggregation-name": {
      "**cardinality**": {
        "field": "customer_id",
        "**precision_threshold**": 3000
      }
    }
  }
}
```

```
{
  "took": 4,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 4675,
      "relation": "eq"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "my-cardianality-aggregation-name": {
      "value": 46
    }
  }
}
```

- precision_threshold  옵션은 정확도를 조절하기 위해 사용한다.
    - 이 값을 높이면 정확도가 올라가지만 메모리를 더 많이 사용한다.
    - precision_threshold가 최종 cardinality보다 높다면 정확도가 충분히 높다.
    - 기본값은 3000이며 최대값은 40000이다.
- 위 요청 “customer_id” 필드의 고유한 값의 갯수를 구하였고 응답으로 46개가 고유한 customer_id라는 것을 알 수 있다.

### 4.4.3 버킷 집계

버킷 집계는 문서를 특정 기준으로 쪼개어 여러 부분 집합으로 나눈다.

이 부분 집합을 버킷이라고 한다.

또한 각 버킷에 포함된문서를 대상으로 별도의 하위 집계(sub-aggregation)를 수행할 수 있다.

> **range 집계**
>

range 집계는 지정한 필드값을 기준으로 문서를 원하는 버킷 구간으로 쪼갠다.

버킷 구간을 나눈 기준이 될필드와 기준 값을 지정해 요청한다.

```
GET kibana_sample_data_flights/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "distance-kilometers-range":{
      "range": { // 구간을 설정 해준다.
        "field": "DistanceKilometers", // 구간을 지정할 필드 지정
        "ranges": [
          {
            "to": 5000
          },
          {
            "from": 5000,
            "to": 10000
          },
          {
            "from": 10000
          }
        ]
      },
      "aggs": {
        "average-ticket-price": { // 각 구간의 버킷 평균 티켓 가격
          "avg": {
            "field": "AvgTicketPrice"
          }
        }
      }
    }
  }
}
```

```
{
  // ...
  "aggregations": {
    "distance-kilometers-range": {
      "buckets": [
        {
          "key": "*-5000.0",
          "to": 5000,
          "doc_count": 4052,
          "average-ticket-price": {
            "value": 513.3930266305937
          }
        },
        {
          "key": "5000.0-10000.0",
          "from": 5000,
          "to": 10000,
          "doc_count": 6042,
          "average-ticket-price": {
            "value": 677.2621444606182
          }
        },
        {
          "key": "10000.0-*",
          "from": 10000,
          "doc_count": 2965,
          "average-ticket-price": {
            "value": 685.3553124773563
          }
        }
      ]
    }
  }
}
```

- range 밑에 aggs가 하나 더 있는 데 이 부분이 하위 집계로 avg 집계를 지정했다.
    - 3개의 버킷 안에 있는 문서를 대상으로 각각 AvgTicketPrice의 평균을 구한다.
- doc_count : 각 버킷의 문서의 수
- 버킷 집계의 활용 핵심은 하위 집계
- 문서 전체 대해 하나의 집계를 수행하는 것이 문서를 여러 구간의 버킷으로 나눈 뒤 각 버킷에 대해서 하위 집계를 수행하도록 하는 것이다.
- 하위 집계를 중첩해서 버킷 집계를 넣으면 그 하위 집계를 지정하는 것도 가능하다.

> **date_range 집계**
>

date_range 집계는 range 집계와 유사하나 date 타입 필드를 대상으로 한다는 것에 차이가 있다.

from과 to에 간단한 날짜 시간 계산식을 사용할 수 있다는 차이가 있다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "date-kilometers-range":{
      "date_range": {
        "field": "order_date",
        "ranges": [
          {
            "to": "now-10d/d"
          },
          {
            "from": "now-10d/d",
            "to": "now"
          },
          {
            "from": "now"
          }
        ]
      }
    }
  }
}
```

- 위 예제에서는 범위를 지정하는 부분에 now가 들어갔는데 now의 경우에는 매 순간마다 값이 달라지므로 엘라스틱서치의 캐싱을 사용하기 힘들다.
    - 캐싱한 값을 사용하기 동일한 집계 요청인지의 여부를 본문의 내용으로 구분하기 때문이다.

> **histogram 집계**
>

histogram 집계는 지정한 필드의 값을 기준으로 버킷을 나눈다는 점에서 range 집계와 유사하다.

다른 점은 버킷 구분의 경계 기준값을 직접 지정하는 것이 아니라 버킷의 간격을 지정해서 경계를 나눈다는 점이 다르다.

```
GET kibana_sample_data_flights/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "my-histogram": {
      "histogram": {
        "field": "DistanceKilometers",
        "interval": 1000
      }
    }
  }
}
```

```
{
  "took": 6,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 10000,
      "relation": "gte"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "my-histogram": {
      "buckets": [
        {
          "key": 0,
          "doc_count": 1806
        },
        {
          "key": 1000,
          "doc_count": 1153
        },
        {
          "key": 2000,
          "doc_count": 530
        },
        {
          "key": 3000,
          "doc_count": 241
        },
        {
          "key": 4000,
          "doc_count": 322
        },
        {
          "key": 5000,
          "doc_count": 414
        },
        {
          "key": 6000,
          "doc_count": 1065
        },
        {
          "key": 7000,
          "doc_count": 1721
        },
        {
          "key": 8000,
          "doc_count": 1348
        },
        {
          "key": 9000,
          "doc_count": 1494
        },
        {
          "key": 10000,
          "doc_count": 650
        },
        {
          "key": 11000,
          "doc_count": 655
        },
        {
          "key": 12000,
          "doc_count": 325
        },
        {
          "key": 13000,
          "doc_count": 302
        },
        {
          "key": 14000,
          "doc_count": 278
        },
        {
          "key": 15000,
          "doc_count": 285
        },
        {
          "key": 16000,
          "doc_count": 328
        },
        {
          "key": 17000,
          "doc_count": 41
        },
        {
          "key": 18000,
          "doc_count": 69
        },
        {
          "key": 19000,
          "doc_count": 32
        }
      ]
    }
  }
}
```

- interval을 지정하면 해당 필드의 최솟값과 최댓값을 확인한 후 그 사이를 interval에 지정한 간격으로 쪼개서 버킷을 나눈다.
- 특별히 지정하지 않으면 기본적으로 0을 기준으로 히스토그램의 계급을 ㅏㄴ눈다.
- 시작 위치를 지정하고 싶으면 offset을 사용할 수 있다.

```
GET kibana_sample_data_flights/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "my-histogram": {
      "histogram": {
        "field": "DistanceKilometers",
        "interval": 1000,
        "offset": 50
      }
    }
  }
}
```

```
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 10000,
      "relation": "gte"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "my-histogram": {
      "buckets": [
        {
          "key": -950,
          "doc_count": 643
        },
        {
          "key": 50,
          "doc_count": 1231
        },
        {
          "key": 1050,
          "doc_count": 1111
        },
        {
          "key": 2050,
          "doc_count": 522
        },
        {
          "key": 3050,
          "doc_count": 232
        },
        {
          "key": 4050,
          "doc_count": 329
        },
        {
          "key": 5050,
          "doc_count": 431
        },
        {
          "key": 6050,
          "doc_count": 1100
        },
        {
          "key": 7050,
          "doc_count": 1732
        },
        {
          "key": 8050,
          "doc_count": 1367
        },
        {
          "key": 9050,
          "doc_count": 1420
        },
        {
          "key": 10050,
          "doc_count": 647
        },
        {
          "key": 11050,
          "doc_count": 650
        },
        {
          "key": 12050,
          "doc_count": 327
        },
        {
          "key": 13050,
          "doc_count": 290
        },
        {
          "key": 14050,
          "doc_count": 280
        },
        {
          "key": 15050,
          "doc_count": 299
        },
        {
          "key": 16050,
          "doc_count": 313
        },
        {
          "key": 17050,
          "doc_count": 34
        },
        {
          "key": 18050,
          "doc_count": 72
        },
        {
          "key": 19050,
          "doc_count": 29
        }
      ]
    }
  }
}
```

- offset으로 지정한 50로 시작하여 구간을 나눠진 걸 확인할 수 있다.
- 이 외에도 [-950,50) 구간이 생긴다.
- 이 밖에도 “mid_doc_count” : 600 설정을 추가하면 doc_count가 지정한 값보다 작은 경우는 제외된다.

> **date_histogram 집계**
>

date_histogram 집계는 histogram 집계와 유사하지만 date 타입 필드를 사용한다는 점이 다르다.

또한 interval 대신 calendar_interval이나 fixed_inteval을 사용한다.

year, quarter, month, week, 같은 달력 기준의 값은 `"calendar_interval" : "month"` 로 입력을 하고, 30일 처럼 정확히 구분되는 날짜들은 `"fixed_interval" : "30d"` 로 지정을 해야 합니다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "my-date-histogram": {
      "date_histogram": {
        "field": "order_date",
        "calendar_interval":"day"
      }
    }
  }
}
```

`calendar_interval` 에서 지정할 수 있는 값

- minute 또는 1m : 분 단위
- hour 또는 1h : 시간 단위
- day 또는 1d : 일 단위
- month 또는 1m : 월 단위
- quarter 또는 1q : 분기 단위
- year 또는 1y : 연 단위

`fixed_interval` 을 사용해야 되는 경우

- 1개 단위가 아닌 값은 `calendar_interval` 에서 못 지정하므로 이런 단위로 버킷을 나누고 싶을 때 사용한다.
- ex) “fixed_interval” : “3h”

date_histogram 집계도 histogram 집계 처럼 offset과 min_doc_count 설정할 수 있다.

> **terms 집계**
>

terms 집계는 지정한 필드에 대해 가장 빈도수가 높은 term 순서대로 버킷을 생성한다.

버킷을 최대 몇 개까지 생성할 것인지를 size로 지정한다.

```
GET kibana_sample_data_logs/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "my-term-histogram": {
      "terms": {
        "field": "host.keyword",
        "size": 10
      }
    }
  }
}
```

```
{
  "took": 5,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 10000,
      "relation": "gte"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "my-term-histogram": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "artifacts.elastic.co",
          "doc_count": 6488
        },
        {
          "key": "www.elastic.co",
          "doc_count": 4779
        },
        {
          "key": "cdn.elastic-elastic-elastic.org",
          "doc_count": 2255
        },
        {
          "key": "elastic-elastic-elastic.org",
          "doc_count": 552
        }
      ]
    }
  }
}
```

- terms 집계는 각 샤드에서 size 개수만큼 term을 뽑아 빈도수를 센다.
- 각 샤드에서 수행된 계산을 한 곳으로 모아 합산한 후 size 개수만큼 버킷을 뽑는다.
- size 개수와 각 문서의 분포에 따라 결과가 정확하지 않을 수 있다.
- 특히 해당 필드의 고유한 term 개수가 size보다 많다면 상위에 뽑혀야할 term이 최종결과에 포함되지 않을 수있다.
- 응답 본문의 `doc_count_error_upper_bound` 는 doc_cout의 오차 상한선을 나타낸다.
    - 이 값이 크다면 size를 높이는 것을 고려할 수 있다.
    - size를 높이면 정확도는 올라가지만 그만큼 성능이 하락한다.
- `sum_other_doc_count` 필드는 최종적으로 버킷에 포함되지 않은 문서의 수를 나타낸다. 즉 상위 term에 들지 못한 문서 개수의 총합이다.
- 모든 term에 대해서 페이지네이션으로 전부 순회하며 집계를 하려고한다면 size를 올리는 것보다 composite 집계를 사용하는 것이 좋다. terms 집계는 기본적으로 상위 term을 뽑아서 집계를 수행하도록 설계되었다.

> **composite 집계**
>

composite 집계는 sources로 지정된 하위 집계의 버킷 전부를 페이지네이션을 이용해서 효율적으로 순회하는 집계다.

또한 sources에 하위 집계를 여러 개 지정한 뒤 조합된 버킷을 생성할 수 있다.

```
GET kibana_sample_data_logs/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "composite-aggs":{
      "composite": {
        "size": 100, 
        "sources": [
          {
            "terms-aggs": {
              "terms": {
                "field": "host.keyword"
              }
            }
          },
          {
            "date_histogram-aggs": {
              "date_histogram": {
                "field": "@timestamp",
                "calendar_interval": "day"
              }
            }
          }
        ]
      }
    }
  }
}
```

- 요청 본문의 composite 아래의 size는 페이지네이션 한 번에 몇 개의 버킷을 반환할 것인가를 지정한다.(위 요청에서는 100개의 버킷 반환)
- sources에는 버킷을 조합하여 순회할 하위 집계를 지정한다.
    - 여기에는 모든 종류의 집계를 하위 집계로 지정할 수 없다.
        - terms, histogram, date_histogram 등 일부 집계만 사용 가능하다.
- terms 집계에는 size를 지정하지 않았는데 이는 composite 집계 자체가 버킷 전체를 순차적으로 방문하는 목적의 집계이기 때문에 terms 집계의 size 개념이 필요 없다.

```
{
	// ...
  "aggregations": {
    "composite-aggs": {
      "**after_key**": {
        "terms-aggs": "cdn.elastic-elastic-elastic.org",
        "date_histogram-aggs": 1710288000000
      },
      "buckets": [
        {
          "key": { // 여러 집계의 결과로 조합된 키
            "terms-aggs": "artifacts.elastic.co",
            "date_histogram-aggs": 1707004800000
          },
          "doc_count": 124
        },
        {
          "key": {
            "terms-aggs": "artifacts.elastic.co",
            "date_histogram-aggs": 1707091200000
          },
          "doc_count": 88
        },
        // ...
        
  }
```

- 응답을 보면 버킷의 키가 여러 집계의 결과로 조합된 것을 확인할 수 있다.
- A 집계의 key가 1, 2고 B 집계의 key가 a, b, c라면 조합은 총 6개가 된다.
- `after_key` 는 확인할 수 있는 조합이 바로 페이지네이션을 위해서 필요한 가장 마지막 버킷의 key다.
    - 이 값을 통해 작업 결과의 다음 페이지를 조회할 수 있다.

```
GET kibana_sample_data_logs/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "composite-aggs":{
      "composite": {
        "size": 100, 
        "sources": [
          {
            "terms-aggs": {
              "terms": {
                "field": "host.keyword"
              }
            }
          },
          {
            "date_histogram-aggs": {
              "date_histogram": {
                "field": "@timestamp",
                "calendar_interval": "day"
              }
            }
          }
        ],
        **"after": {
          "terms-aggs":"cdn.elastic-elastic-elastic.org",
          "date_histogram-aggs" : 1710288000000
        }**
      }
    }
  }
}
```

### 4.4.4 파이프라인 집계

파이프라인 집계는 문서나 필드의 내용이 아니라 **다른 집계 결과를 집계 대상**으로 한다.

즉 다른 집계의 결과를 입력값으로 가져와서 작업을 수행한다.

주로 buckets_path라는 인자를 통해 다른 집계의 결과를 가져오며, 이 buckets_path는 상대 경로로 지정한다.

- > : 하위 집계로 이동하는 구분자
- . : 하위 메트릭으로 이동하는 구분자
- 집계 이름
- 메트릭 이름

또한 buckets_path에서는 > 또는 . 문자를 통해 하위 경로로 이동할 수 있으나 상위 경로로는 이동할 수 없다.

> comulative_sum 집계
>

comulative_sum 집계는 다른 집계의 값을 누적하여 합산한다.

buckets_path로 누적 합산할 집계의 이름을 지정한다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "daily-timestamp-bucket": {
      "date_histogram": {
        "field": "order_date",
        "calendar_interval": "day"
      },
      "aggs": {
        "daily-total-quantity-average": {
          "avg": {
            "field": "total_quantity"
          }
        },
        **"pipeline-sum":{
          "cumulative_sum": {
            "buckets_path": "daily-total-quantity-average"
          }
        }**
      }
    }
  }
}
```

```
{
	//...
	"aggregations": {
	    "daily-timestamp-bucket": {
	      "buckets": [
	        {
	          "key_as_string": "2024-02-01T00:00:00.000Z",
	          "key": 1706745600000,
	          "doc_count": 146,
	          "daily-total-quantity-average": {
	            "value": 2.1780821917808217
	          },
	          **"pipeline-sum": {
	            "value": 2.1780821917808217
	          }**
	        },
	        {
	          "key_as_string": "2024-02-02T00:00:00.000Z",
	          "key": 1706832000000,
	          "doc_count": 153,
	          "daily-total-quantity-average": {
	            "value": 2.183006535947712
	          },
	          **"pipeline-sum": {
	            "value": 4.361088727728534
	          }**
	        },
```

- total_quantity의 평균을 구하기 위해 date_histogram으로 버킷을 나눈 뒤 그 하위 집계로 avg 집계를 지정했다.
- 그리고 date_histogram의 하위 집계로 cumulative_sum 집계를 추가 지정했다.
- cumulative_sum의 bucket_path에서는 누적 합산을 수행할 집계로 daily-total-quantity-average를 지정했다.
- cumulative_sum을 수행할 때마다 daily-total-quantity-average를 찾아 그 합을 누적 합산한다.
- 응답 본문의 pipeline-sum로 누적 합산결과를 확인할 수 있다.

> **max_bucket 집계**
>

max_bucket 집계는 다른 집계를 받아서 그 결과가 가장 큰 버킷의 key의 결괏값을 구하는 집계다.

다음은 total_quantity의 일 단위 평균이 가장 큰 날짜와 그 평균을 구하는 예시다.

```
GET kibana_sample_data_ecommerce/_search
{
  "size": 0,
  "query": {
    "match_all": {}
  },
  "aggs": {
    "daily-timestamp-bucket": {
      "date_histogram": {
        "field": "order_date",
        "calendar_interval": "day"
      },
      "aggs": {
        "daily-total-quantity-average": {
          "avg": {
            "field": "total_quantity"
          }
        }
      }
    },
    "max-total-quantity":{
      "max_bucket": {
        "buckets_path": "daily-timestamp-bucket>daily-total-quantity-average"
      }
    }
  }
}
```

```
       
        {
          "key_as_string": "2024-03-02T00:00:00.000Z",
          "key": 1709337600000,
          "doc_count": 142,
          "daily-total-quantity-average": {
            "value": 2.183098591549296
          }
        }
      ]
    },
    "max-total-quantity": {
      "value": 2.289473684210526,
      "keys": [
        "2024-03-01T00:00:00.000Z"
      ]
    }
  }
```

- 요청을 보면 bucket_path 부분에 > 를 통해 상위 버킷에서 하위 버킷으로 경로로 지정했다.
    - 값을 끌어올 집계 이름을 바로 명시하지 않은 이유는 bucket_path 는 상대 경로를 사용하기 때문이다.
    - max_bucket 집계가 위치한 계층에서는 daily-total-quantity-average를 인식할 수 없으므로 daily-timestamp-bucket에서 타고 들어가서 동작하게 한 것이다.