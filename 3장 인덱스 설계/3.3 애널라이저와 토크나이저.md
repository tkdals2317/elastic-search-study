## 3.3 애널라이저와 토크나이저

애널라이저는 0개 이상의 캐릭터 필터, 1개의 토크나이저, 0개 이상의 토큰 필터로 구성되고 순서대로 수행한다.

- 캐릭터 필터 : 입력한 텍스트에 문자열을 변형
- 토크나이저 : 여러 개의 토큰으로 쪼개는 역할
- 토큰 필터 : 쪼개진 토큰의 스트림에 토큰에 특정한 변형하여 최종적으로 분석 완료된 텀 생성

### 3.3.1 analyze API

애널라이저와 각 구성 요소의 동작을 테스트 할 수 있는 API

```
GET _analyze
POST _analyze
```

```
POST _analyze
{
  "analyzer": "standard",
  "text": "Hello, HELLO, World!"
}
```

```
{
  "tokens": [
    {
      "token": "hello",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "hello",
      "start_offset": 7,
      "end_offset": 12,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "world",
      "start_offset": 14,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 2
    }
  ]
}
```

최종적으로 hello, hello, world 토큰으로 쪼개진 것을 확인할 수 있다.

### 3.3.2 캐릭터 필터

텍스트를 캐릭터 스트림으로 받아 특정한 문자를 추가, 변경, 삭제한다.

0개 이상의 캐릭터 필터를 지정할 수 있다.

**내장 빌트인 캐릭터 필터의 종류**

- **HTML strip 캐릭터 필터** : <b>와 같은 HTML 요소 안쪽의 데이터를 꺼낸다. &apos;와 같은 HTML 엔티티도 디코딩한다.
- **mapping 캐릭터 필터** : 치환할 대상이 되는 문자와 치환 문자를 맵 형태로 선언한다.
- **pattern replace 캐릭터 필터** : 정규 표현식을 이용해서 문자를 치환한다.

HTML strip 캐릭터 필터를 anlayze API로 테스트 해보자

```
POST _analyze
{
  "char_filter": ["html_strip"],
  "text": "<p>I&apos;m so <b>happy</b>!</p>"
}
```

```
{
  "tokens": [
    {
      "token": """
I'm so happy!
""",
      "start_offset": 0,
      "end_offset": 32,
      "type": "word",
      "position": 0
    }
  ]
}
```

<p></p>가 줄바꿈 문자로 치환됐고, <b></b>가 제거되고, &apos;가 홑따옴표로 디코됭 되었다.

### 3.3.3 토크나이저

캐릭터 스트림을 받아서 여러 토큰으로 쪼개어 토큰 스트림을 만든다.

애널라이저에는 한 개의 토크나이저를 지정할 수 있다.

**내장 토크나이저의 종류**

> **standard 토크나이저**
>

가장 기본적인 토크나이저로 Unicode Text Segmentation 알고리즘을 사용하여 텍스트를 단어 단위로 나눈다.

대부분의 문장부호가 사라진다.

기본값으로 지정되는 standard 애널라이저가 standard 토크나이저를 이용한다.

> **keyword 토크나이저**
>

들어온 텍스트를 쪼개지 않고 그대로 내보낸다. 즉 커다란 단일 토큰을 내보낸다.

```
POST _analyze
{
  "tokenizer": "keyword",
  "text": "Hello, HELLO, World!"
}
```

```
{
  "tokens": [
    {
      "token": "Hello, HELLO, World!",
      "start_offset": 0,
      "end_offset": 20,
      "type": "word",
      "position": 0
    }
  ]
}
```

쓸모 없어 보일 수 있지만 여러 캐릭터 필터, 토큰 필터와 함께 조합하면 다양한 커스텀 애널라이저 지정이 가능하다.

> **ngram 토크나이저**
>

텍스트를 min_gram 값 이상, max_gram 값 이하의 단위로 쪼갠다.

```
POST _analyze
{
  "tokenizer": {
    "type": "ngram",
    "min_gram": 3,
    "max_gram": 4
  },
  "text": "Hello, World!"
}
```

```
{
  "tokens": [
    {
      "token": "Hel",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "Hell",
      "start_offset": 0,
      "end_offset": 4,
      "type": "word",
      "position": 1
    },
    {
      "token": "ell",
      "start_offset": 1,
      "end_offset": 4,
      "type": "word",
      "position": 2
    },
    {
      "token": "ello",
      "start_offset": 1,
      "end_offset": 5,
      "type": "word",
      "position": 3
    },
    {
      "token": "llo",
      "start_offset": 2,
      "end_offset": 5,
      "type": "word",
      "position": 4
    },
    {
      "token": "llo,",
      "start_offset": 2,
      "end_offset": 6,
      "type": "word",
      "position": 5
    },
    {
      "token": "lo,",
      "start_offset": 3,
      "end_offset": 6,
      "type": "word",
      "position": 6
    },
    {
      "token": "lo, ",
      "start_offset": 3,
      "end_offset": 7,
      "type": "word",
      "position": 7
    },
    {
      "token": "o, ",
      "start_offset": 4,
      "end_offset": 7,
      "type": "word",
      "position": 8
    },
    {
      "token": "o, W",
      "start_offset": 4,
      "end_offset": 8,
      "type": "word",
      "position": 9
    },
    {
      "token": ", W",
      "start_offset": 5,
      "end_offset": 8,
      "type": "word",
      "position": 10
    },
    {
      "token": ", Wo",
      "start_offset": 5,
      "end_offset": 9,
      "type": "word",
      "position": 11
    },
    {
      "token": " Wo",
      "start_offset": 6,
      "end_offset": 9,
      "type": "word",
      "position": 12
    },
    {
      "token": " Wor",
      "start_offset": 6,
      "end_offset": 10,
      "type": "word",
      "position": 13
    },
    {
      "token": "Wor",
      "start_offset": 7,
      "end_offset": 10,
      "type": "word",
      "position": 14
    },
    {
      "token": "Worl",
      "start_offset": 7,
      "end_offset": 11,
      "type": "word",
      "position": 15
    },
    {
      "token": "orl",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 16
    },
    {
      "token": "orld",
      "start_offset": 8,
      "end_offset": 12,
      "type": "word",
      "position": 17
    },
    {
      "token": "rld",
      "start_offset": 9,
      "end_offset": 12,
      "type": "word",
      "position": 18
    },
    {
      "token": "rld!",
      "start_offset": 9,
      "end_offset": 13,
      "type": "word",
      "position": 19
    },
    {
      "token": "ld!",
      "start_offset": 10,
      "end_offset": 13,
      "type": "word",
      "position": 20
    }
  ]
}
```

21개의 토큰으로 쪼개졌구 그중에는 “o, W”나 “, Wo”나 “ld!”처럼 공백문자나 문장 부호가 포함되어 사실상 활용 의미가 없는 토큰도 포함됐다.

이런 문제를 해결하기 위해 token_char라는 속성을 통해 토큰에 포함시킬 타입 문자를 지정할 수 있다.

**token_char에 포함 시킬 수 있는 값**

| 종류 | 설명 |
| --- | --- |
| Letter | 언어의 글자로 분류되는 문자 |
| Digit | 숫자로 분류되는 문자 |
| whitespace | 띄어쓰기나 줄바꿈 문자 등 공백으로 인식되는 문자 |
| punctuation | !나 “등 문장 부호 |
| symbol | √나 $와 같은 기호 |
| custom | custom_token_char 설정을 통해 따로 지정한 커스텀 문자 |

```
POST _analyze
{
  "tokenizer": {
    "type": "ngram",
    "min_gram": 3,
    "max_gram": 4,
    "token_chars": ["letter"]
  },
  "text": "Hello, World!"
}
```

```
{
  "tokens": [
    {
      "token": "Hel",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "Hell",
      "start_offset": 0,
      "end_offset": 4,
      "type": "word",
      "position": 1
    },
    {
      "token": "ell",
      "start_offset": 1,
      "end_offset": 4,
      "type": "word",
      "position": 2
    },
    {
      "token": "ello",
      "start_offset": 1,
      "end_offset": 5,
      "type": "word",
      "position": 3
    },
    {
      "token": "llo",
      "start_offset": 2,
      "end_offset": 5,
      "type": "word",
      "position": 4
    },
    {
      "token": "Wor",
      "start_offset": 7,
      "end_offset": 10,
      "type": "word",
      "position": 5
    },
    {
      "token": "Worl",
      "start_offset": 7,
      "end_offset": 11,
      "type": "word",
      "position": 6
    },
    {
      "token": "orl",
      "start_offset": 8,
      "end_offset": 11,
      "type": "word",
      "position": 7
    },
    {
      "token": "orld",
      "start_offset": 8,
      "end_offset": 12,
      "type": "word",
      "position": 8
    },
    {
      "token": "rld",
      "start_offset": 9,
      "end_offset": 12,
      "type": "word",
      "position": 9
    }
  ]
}
```

token_chars를 “letter”로 지정하면 10개의 토큰으로 쪼개지고 공백 문자를 전후해 위치한 문자로 구성된 “oWo” 같은 토큰은 포함되지 않았다.

ngram 토크나이저는 엘라스틱서치에서 RDB의 ‘LIKE \*검색어\*’와 유사한 검색을 구현하고 싶을 때,

자동 완성 관련 서비스를 구현하고 싶을 때 주로 활용한다.

min_gram과 max_gram 값의 차이가 2 이상으로 벌어지면 분석 시도가 실패하게 되는데, 이 제한값은

**index.max_ngram_diff** 인덱스 설정을 통해 지정할 수 있으며 기본값은 1이다.

이 값은 인덱스 설정이므로 인덱스를 생성할 때 지정해야 한다.

인덱스를 지정하지 않고 사용하는 analyze API로 변경하며 테스트 할 수 없으므로 인덱스를 생성한 뒤 해당 인덱스를 대상으로 analyze API를 사용해야 한다.

> **edge_ngram 토크나이저**
>

ngram 토크나이저와 유사한 동작을 한다.

입력된 텍스트를 token_chars에 지정되지 않은 문자를 기준으로 삼아 단어 단위로 쪼갠다.

각 단어를 min_gram 값 이상, max_gram 값 이하의 문자 길이를 가진 토큰으로 쪼갠다.

**ngram 토크나이저와 다르게 생성된 모든 토큰의 시작 글자를 단어의 시작 글자로 고정시켜서 생성한다.**

```
POST _analyze
{
  "tokenizer": {
    "type": "edge_ngram",
    "min_gram": 3,
    "max_gram": 4,
    "token_chars": ["letter"]
  },
  "text": "Hello, World!"
}
```

```
{
  "tokens": [
    {
      "token": "Hel",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "Hell",
      "start_offset": 0,
      "end_offset": 4,
      "type": "word",
      "position": 1
    },
    {
      "token": "Wor",
      "start_offset": 7,
      "end_offset": 10,
      "type": "word",
      "position": 2
    },
    {
      "token": "Worl",
      "start_offset": 7,
      "end_offset": 11,
      "type": "word",
      "position": 3
    }
  ]
}
```

단어의 시작 글자로 시작하는 4개의 토큰이 생성된다.

> **그 외 토크나이저**
>
- letter 토크나이저 : 공백, 특수문자등 언어의 글자로 분류되는 문자가 아닌 문자를 만났을 때 쪼갠다.
- whitespace 토크나이저 : 공백 문자를 만났을 때 쪼갠다.
- pattern 토크나이저 : 지정한 정규표현식을 단어의 구분자로 사용하여 쪼갠다.

### 3.3.4 토큰 필터

**토큰 스트림**을 받아서 **토큰을 추가, 변형, 삭제**한다.

하나의 애널라이저에 토큰 필터를 0개 이상 지정할 수 있고 여러 개의 필터가 있는 경우 순차적으로 적용된다.

**내장 토큰 필터 종류**

- lowercase / uppercase 토큰 필터 : 토큰의 내용을 소문자/대문자로 만들어준다.
- stop 토큰 필터 : 불용어를 지정하여 제거할 수 있다. ex) the, a, an, in
- synonym 토큰 필터 : 유의어 사전 파일을 지정하여 지정된 유의어를 치환한다.
- pattern_replace 토큰 필터 : 정규식을 사용하여 토큰의 내용을 치환한다.
- stemmer 토큰 필터 : 지원되는 몇몇 언어의 어간 추출을 수행한다. 한국어는 지원 X
- trim 토큰 필터 : 토큰의 전후에 위치한 공백 문자를 제거한다.
- truncate 토큰 필터 : 지정한 길이의 토큰을 자른다.

예제로 테스트를 해보자

```
POST _analyze
{
  "filter": ["lowercase"],
  "text": "Hello, World!"
}
```

```
{
  "tokens": [
    {
      "token": "hello, world!",
      "start_offset": 0,
      "end_offset": 13,
      "type": "word",
      "position": 0
    }
  ]
}
```

### 3.3.5 내장 애널라이저

엘라스틱서치에는 내장 캐릭터 필터, 토크나이저, 토큰 필터를 조합하여 미리 만들어놓은 다양한 내장 애널라이저가 있다.

- stardard 애널라이저 : standard 토크나이저와 lowercase 토큰 필터로 구성, 기본 애널라이저
- simple 애널라이저 : letter가 아닌 문자 단위로 쪼갠 뒤 lowercase 토큰 필터를 적용한다.
- whitespace 애널라이저 : whitespace 토크나이저로 구성, 즉 공백 문자 단위로 토큰을 쪼갠다.
- stop 애널라이저 : standard와 동일한 내용이지만 stop 토큰 필터를 적용해 불용어를 제거한다.
- keyword 애널라이저 : keyword 토크나이저로 구성되고, 하나의 큰 토큰을 그대로 반환한다.
- pattern 애널라이저 : pattern 토크나이저와 lowercase 토큰 필터로 구성된다.
- language 애널라이저 : 여러 언어의 분석을 지원하나 한국의 분석은 지원하지 않는다.
- fingerprint 애널라이저 : 중복 검출에 사용할 수 있는 특별한 핑거프린트용 토큰 생성, standard 토크나이저 적용 후, lowercase 토큰 필터, ASCII folding 토큰 필터(ASCII 내에 동격인 문자가 있는 경우 동격인 ASCII 문자로 치환), stop 필터(비활성화), fingerprint 토큰 필터(토큰 정렬 후 중복 제거 후 단일 토큰으로 합침)를 차례로 적용

fingerprint 애널라이저를 테스트 해보자

```
POST _analyze
{
  "analyzer": "fingerprint",
  "text": "Yes yes, Gödel said this sentence is consistent and."
}
```

```
{
  "tokens": [
    {
      "token": "and consistent godel is said sentence this yes",
      "start_offset": 0,
      "end_offset": 52,
      "type": "fingerprint",
      "position": 0
    }
  ]
}
```

Gödel은 godel로 변환되었고 중복되는 Yes와 yes는 통일되고 정렬되어서 토큰이 결과로 나온 모습을 확인할 수 있다.

### 3.3.6 애널라이저를 매핑에 적용

실제 각 필드의 매핑에 애널라이저를 적용하는 방법을 알아본다.

다음과 같이 새 인덱스를 생성해 보자

```
PUT analyzer_test
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "type": "keyword"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "defaultText": {
        "type": "text"
      },
      "standardText": {
        "type": "text",
        "analyzer": "standard"
      }
    }
  }
}

```

index.settings.anaysis.analyzer 설정에는 커스텀 애널라이저를 추가할 수 있다.

여기서 default라는 이름으로 애널라이저를 지정하면 기본 애널라이저를 변경할 수 있다.

위 예제에서는 기본값인 standard 애널라이저를 keyword 애널라이저로 변경했다.

그 외 매핑 부분에서 text 타입의 defaultText 필드와 standardText 필드를 지정했다.

defaultText 필드 → keyword 애널라이저, standardText 필드 → standard 애널라이저가 적용된다.

### 3.3.7 커스텀 애널라이저

캐릭터 필터, 토크나이저, 토큰 필터를 원하는 대로 조합해 지정해보자

```
PUT analyzer_test2
{
  "settings": {
    "analysis": {
        "char_filter": {
          "my_char_filter":{
            "type": "mapping",
            "mappings": [
              "i. => 1.",
              "ii. => 2.",
              "iii. => 3.",
              "iv. => 4."
            ]
          }
        },
        "analyzer": {
          "my_analyzer": {
            "char_filter": [
              "my_char_filter"
            ],
            "tokenizer": "whitespace",
            "filter": [
              "lowercase"
            ]
          }
        }
    }
  },
  "mappings": {
    "properties": {
      "myText": {
        "type": "text",
        "analyzer": "my_analyzer"
      }
    }
  }
}

```

my_char_filter라는 커스텀 캐릭터 필터를 선언하고 whitespace 토크나이저, lowercase 토큰 필터를 조합해 my_analyzer라는 커스텀 애널라이저를 선언했다.

이 인덱스의 이름을 지정하여 analyze API를 사용해보자

```
GET [인덱스 이름]/_analyze
POST [인덱스 이름]/_analyze
```

```
GET analyzer_test2/_analyze
{
  "analyzer": "my_analyzer",
  "text":"i.hello ii.World iii.Bye, iv.World!"
}
```

```
{
  "tokens": [
    {
      "token": "1.hello",
      "start_offset": 0,
      "end_offset": 7,
      "type": "word",
      "position": 0
    },
    {
      "token": "2.world",
      "start_offset": 8,
      "end_offset": 16,
      "type": "word",
      "position": 1
    },
    {
      "token": "3.bye,",
      "start_offset": 17,
      "end_offset": 25,
      "type": "word",
      "position": 2
    },
    {
      "token": "4.world!",
      "start_offset": 26,
      "end_offset": 35,
      "type": "word",
      "position": 3
    }
  ]
}
```

### 3.3.8 플러그인 설치를 통한 애널라이저 추가와 한국어 형태소 분석

한국어 형태소 분석을 지원하는 기본 내장 애널라이저는 없다.

하지만 엘라스틱서치가 공식 제공하는 nori 플러그인을 설치하면 한국어를 분석할 수 있다.

```bash
# 윈도우 기준으로 .bat을 붙혀준다.
$ bin/elasticsearch-plugin.bat install analysis-nori # 한국어 플러그인
$ bin/elasticsearch-plugin.bat install analysis-kuromoji # 일본어 플러그인
$ bin/elasticsearch-plugin.bat install analysis-smartcn # 중국어 플러그인
```

클러스터를 구성하고 있는 모든 노드에 설치해주고 설치 후 에는 다시 엘라스틱서치 클러스터를 재기동 시켜야 적용이 된다.

```
POST _analyze
{
  "analyzer": "nori",
  "text": "우리는 컴퓨터를 다룬다."
}
```

```
{
  "tokens": [
    {
      "token": "우리",
      "start_offset": 0,
      "end_offset": 2,
      "type": "word",
      "position": 0
    },
    {
      "token": "컴퓨터",
      "start_offset": 4,
      "end_offset": 7,
      "type": "word",
      "position": 2
    },
    {
      "token": "다루",
      "start_offset": 9,
      "end_offset": 12,
      "type": "word",
      "position": 4
    }
  ]
}
```

애널라이저가 조사를 제거했고 “다룬다”의 어간인 “다루”를 제대로 분리했다.

### 3.3.9 노멀라이저

적용대상이 text 타입이 아닌 keyword 타입의 필드에 적용되고 애널라이저와 다르게 단일 토큰을 생성한다.

노멀라이저는 토크나이저 없이 캐릭터 필터, 토큰 필터로 구성된다.

최종적으로 단일 토큰을 생성해야 하기 때문에 ASCII folding, lowercase, uppercase 등 글자 단위로 작업을 수행하는 필터만 사용할 수 있다.

```
PUT normalizer_test
{
  "settings": {
    "analysis": {
      "normalizer":{
        "my_normalizer":{
          "type": "custom",
          "char_filter":[],
          "filter":[
            "asciifolding",
            "uppercase"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "myNormalizerKeyword": {
        "type": "keyword",
        "normalizer": "my_normalizer"
      },
      "lowercaseKeyword": {
        "type": "keyword",
        "normalizer":"lowercase"
      },
      "defaultKeyword": {
        "type": "keyword"
      }
    }
  }
}
```

```
GET normalizer_test/_analyze
{
  "field": "myNormalizerKeyword",
  "text": "Häppy Wörld!!"
}

GET normalizer_test/_analyze
{
  "field": "lowercaseKeyword",
  "text": "Häppy Wörld!!"
}

GET normalizer_test/_analyze
{
  "field": "defaultKeyword",
  "text": "Häppy Wörld!!"
}
```

```
{
  "tokens": [
    {
      "token": "HAPPY WORLD!!",
      "start_offset": 0,
      "end_offset": 13,
      "type": "word",
      "position": 0
    }
  ]
}
{
  "tokens": [
    {
      "token": "häppy wörld!!",
      "start_offset": 0,
      "end_offset": 13,
      "type": "word",
      "position": 0
    }
  ]
}
{
  "tokens": [
    {
      "token": "Häppy Wörld!!",
      "start_offset": 0,
      "end_offset": 13,
      "type": "word",
      "position": 0
    }
  ]
}
```

---